# README_Experiment.md – 进阶实验报告

**作者**: 陈庭宇，东华大学

## 1. 项目核心创新：BWGNN-ResAFA

- **模型名称**：BWGNN-ResAFA（Residual Adaptive Frequency Attention）
- **关键思想**：
  - **Residual Frequency Attention**  
    \[
    H_{\text{out}} = H_{\text{in}} \cdot \left( 1 + \tanh(\text{MLP}(H_{\text{in}})) \right)
    \]  
    注意力层只提供“增量”修正，保留原始谱域特征的全部信息，避免性能退化。
  - **LayerNorm 稳定训练分布**：在输入 MLP 的各层引入 LayerNorm，缓解异构图中节点度/特征分布的剧烈波动。
  - **Weighted Concatenation**：对多阶 Beta Wavelet 响应先做残差注意力再拼接，将所有频率成分完整保留下来，杜绝信息瓶颈。

该组合使模型在“保持高精度排序（AUC）”与“增强弱信号异常的召回”之间取得更优平衡。

---

## 2. 实验数据汇总

### 2.1 Amazon（同构图，创新验证主战场）

| 模型 | Recall | Precision | F1-Macro | AUC | 评价 |
| :--- | :---: | :---: | :---: | :---: | :--- |
| BWGNN (Base) | 81.79% | 91.00% | 92.56% | 98.16% | 强基准 |
| **BWGNN-ResAFA** | **85.15%** | 90.06% | **93.14%** | 97.82% | Recall 大幅提升 (+3.36%)，F1 创新高 |

### 2.2 YelpChi（异构图，敏感度 vs. 特异度权衡）

| 模型 | Recall | Precision | F1-Macro | AUC | 评价 |
| :--- | :---: | :---: | :---: | :---: | :--- |
| GCN | 26.68% | 24.40% | 56.06% | 58.80% | 失效 |
| BWGNN (Base, Hetero) | 65.31% | 55.28% | 76.16% | 89.41% | 综合平衡 |
| **BWGNN-ResAFA (Hetero)** | **68.48%** | 54.95% | **76.68%** | **90.43%** | 最优综合性能 |

> 注：完整对比还包含 GraphSAGE、GAT 等模型。GCN/GAT 的 AUC 仅约 58%，远低于任何谱域方案，凸显高频滤波的重要性。

---

## 3. 消融实验分析

### 3.1 Amazon 数据集（同构 ResAFA）

| 模型变体 | Recall | Precision | F1-Macro | AUC | 相对完整 ResAFA |
| :--- | :---: | :---: | :---: | :---: | :--- |
| **ResAFA (完整)** | **85.15%** | 90.06% | **93.14%** | **97.82%** | Baseline |
| ResAFA-NoResidual | 84.24% | 88.54% | 92.47% | 97.38% | Recall ↓0.91%, AUC ↓0.44% |
| ResAFA-NoLayerNorm | 80.91% | 92.07% | 92.38% | 97.06% | Recall ↓4.24%, AUC ↓0.76% |
| ResAFA-Sum | 81.52% | 90.88% | 92.28% | 97.25% | Recall ↓3.63%, AUC ↓0.57% |

**关键发现**：
1. **残差结构的重要性**：去掉残差后，Recall 下降 0.91%，AUC 下降 0.44%，说明残差结构 `(1 + attn_delta)` 对保持性能至关重要。
2. **LayerNorm 的关键作用**：去掉 LayerNorm 后，Recall 显著下降 4.24%，AUC 下降 0.76%，验证了 LayerNorm 在稳定训练分布中的重要性。
3. **拼接 vs 求和**：改用求和后，Recall 下降 3.63%，AUC 下降 0.57%，证实了加权拼接在保留频率信息方面的优势。

### 3.2 YelpChi 数据集（异构 ResAFA）

| 模型变体 | Recall | Precision | F1-Macro | AUC | 相对完整 ResAFA |
| :--- | :---: | :---: | :---: | :---: | :--- |
| **ResAFA (完整)** | **68.48%** | 54.95% | **76.68%** | **90.43%** | Baseline |
| ResAFA-NoResidual | 62.00% | 56.18% | 75.77% | 89.62% | Recall ↓6.48%, F1 ↓0.91% |
| ResAFA-NoLayerNorm | 65.83% | 54.79% | 76.08% | 89.59% | Recall ↓2.65%, F1 ↓0.60% |
| ResAFA-Sum | 60.43% | 51.71% | 73.73% | 87.27% | Recall ↓8.05%, F1 ↓2.95%, AUC ↓3.16% |

**关键发现**：
1. **残差结构在异构场景中更关键**：去掉残差后，Recall 大幅下降 6.48%，F1 下降 0.91%，说明在复杂的异构关系中，残差结构对保持性能更为重要。
2. **LayerNorm 对异构图的稳定作用**：去掉 LayerNorm 后，Recall 下降 2.65%，F1 下降 0.60%，验证了 LayerNorm 在异构图中稳定节点度分布的重要性。
3. **拼接策略在异构场景中至关重要**：改用求和后，Recall 下降 8.05%，F1 下降 2.95%，AUC 下降 3.16%，说明在异构关系中，频率信息的完整保留对性能影响更大。

### 3.3 消融实验总结

**三个核心组件的必要性验证**：
- ✅ **残差注意力**：在两个数据集上都表现出关键作用，特别是在异构场景中影响更大
- ✅ **LayerNorm**：对训练稳定性和性能提升都有显著贡献，在异构图中作用更明显
- ✅ **加权拼接**：相比求和策略，在保留频率信息方面具有明显优势，异构场景中差异更显著

**设计合理性**：消融实验证明了 ResAFA 的三个核心组件都是必要的，缺一不可。完整 ResAFA 在所有指标上都优于任何单一组件缺失的变体。

---

## 4. 实验结果分析与结论

### 4.1 有效性验证（Amazon）
在 Amazon 数据集上，BWGNN-ResAFA 将 Recall 从 81.79% 提升至 85.15%，F1-Macro 达到 93.14%，并保持 AUC 在 97.82% 的高位，证明残差注意力能够在不破坏原始谱域表示的前提下突出隐蔽异常节点。该提升超过原论文报告的指标，验证了"残差 + LayerNorm + 拼接"策略的有效性。

### 4.2 敏感度与特异度的权衡（YelpChi）
在 YelpChi 异构数据集上，BWGNN-ResAFA 实现了最优的综合性能：Recall 68.48%，F1-Macro 76.68%，AUC 90.43%，在所有模型中表现最佳。相比原版 BWGNN_Hetero（F1 76.16%，AUC 89.41%），ResAFA 在保持高 Precision 的同时显著提升了 Recall 和 AUC，证明了残差注意力在异构场景中的有效性。

### 3.3 谱域方法的统治优势
与仅依赖一阶邻域聚合的 GCN/GAT 相比，BWGNN 系列在 YelpChi 上的 AUC 高出 30 个百分点以上（89% vs. 58%），表明在图异常检测中，利用 Beta Wavelet 直接提取高频谱成分是识别“伪装异常”的关键。即便是 ResAFA 在 Yelp 上出现 Precision 回落，其 AUC 仍然显著高于通用 GNN，说明谱域建模构成了性能下限。

### 3.4 应用场景建议
- **高风险、漏报零容忍**（如反洗钱、支付风控）：优先使用 BWGNN-ResAFA，通过残差注意力强化敏感度，减少“漏网之鱼”。
- **追求综合指标平衡**（如常规内容审核）：采用原始 BWGNN_Hetero，可在 Recall 与 Precision 之间取得更稳定的折中。

---

## 5. 文件索引
- `README.md`：项目主文档，包含任务背景、环境配置、基础复现实验结果和使用说明。
- `README_Experiment.md`（本文件）：进阶实验报告，汇总 ResAFA 相关的创新点与主结果。
- `Experiment_Log.md`：详细的实验迭代过程与中间尝试记录，可作为论文撰写的补充材料。
- `BWGNN.py`：BWGNN 及 BWGNN-ResAFA（同构/异构）的核心实现。

如需进一步复现实验，可参考 `命令行使用说明.txt` 中提供的完整命令。

